# ðŸ“Œ Open-Source vs. Enterprise Solutions for PDF Processing & Web Scraping

## ðŸ“– Project Summary
This project aims to compare and document the efficiency, accuracy, and scalability of **open-source vs. enterprise solutions** for **PDF processing and web scraping**, with data stored in an **AWS S3 bucket**. The project will evaluate performance differences between **PyMuPDF vs. Azure AI Document Intelligence** for PDF processing and **Selenium vs. ScrapingBee** for web scraping.



---

## ðŸ“Œ Problem Statement
The objective of this assignment is to **evaluate and compare open-source and enterprise solutions** for **PDF processing and web scraping** by developing a structured documentation framework and storing the extracted data in an **AWS S3 bucket**.

---

## ðŸŽ¯ Project Goal
The goal of this project is to **compare and document the efficiency of open-source vs. enterprise solutions** by generating structured datasets. The project will:
- **Extract and structure data from PDFs and websites** using **open-source** (PyMuPDF, Selenium) and **enterprise solutions** (Azure AI Document Intelligence, ScrapingBee).
- **Create well-organized datasets** from the extracted data for further analysis.
- **Provide a detailed comparative analysis** covering **performance, accuracy, cost, and scalability trade-offs**.
- **Document the entire process in a structured GitHub repository** with a **proof-of-concept demo and AI Use Disclosure**.

---

## ðŸ› ï¸ Technology Used

### ðŸŒ Frontend:
- **Streamlit** (For interactive UI)

### ðŸš€ Backend:
- **FastAPI** (For API service)
- **FastAPI Deployment:** Render

### ðŸ“‚ APIs & Tools:
#### **ðŸ“„ PDF Processing**
- **PyMuPDF** (Open-source)
- **Azure AI Document Intelligence** (Enterprise)

#### **ðŸŒ Web Scraping**
- **BeautifulSoup** (Open-source)
- **ScrapingBee** (Enterprise)

#### **ðŸ“œ Documentation & Visualization**
- **Markdown** (For structured documentation)
- **Docking** (For structured documentation management)
- **GitHub Pages** (For hosting documentation)

---

## ðŸ”¬ Proof of Concept (PoC)
To validate the feasibility of using **open-source and enterprise tools** for **PDF processing and web scraping**, the following tasks will be performed:

âœ… **PDF Extraction Tests:**
- Use **PyMuPDF** to extract text and images from PDFs and store results in a structured format.
- Use **Azure AI Document Intelligence** to process complex documents and compare accuracy.

âœ… **Web Scraping Tests:**
- Use **BeautifulSoup** to automate the extraction of structured data from websites.
- Use **ScrapingBee** to test large-scale, API-based web scraping.

âœ… **Comparative Analysis:**
- Measure **processing speed, accuracy, scalability, and ease of use**.
- Document findings in a **side-by-side comparison** with pros and cons.

âœ… **Final Demonstration:**
- Showcase results through a **demo video, GitHub repository, and AI Use Disclosure**.

---

## ðŸ—ï¸ Architecture Diagram
![ai_application_data_pipeline](https://github.com/user-attachments/assets/540fa59f-6a16-461d-965f-8ce68f3bbda5)


---

## ðŸ”‘ Features & How to Run Locally

### âœ… Key Features
âœ” **Open-Source vs. Enterprise Comparison** â€“ Evaluate trade-offs between **cost, performance, and scalability**.
âœ” **PDF Text & Data Extraction** â€“ Process **structured/unstructured** PDF content.
âœ” **Web Scraping Automation** â€“ Extract structured data from websites efficiently.
âœ” **Detailed Documentation** â€“ Step-by-step setup, API usage, and findings.

### ðŸ—ï¸ How to Run the Application Locally
```bash
# 1ï¸âƒ£ Create a Python Virtual Environment
python -m venv venv

# 2ï¸âƒ£ Activate Virtual Environment
source venv/bin/activate  # On Mac/Linux
venv\Scripts\activate  # On Windows

# 3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Start FastAPI Backend
uvicorn backend.app:app --reload

# 5ï¸âƒ£ Start Streamlit Frontend
streamlit run frontend/dashboard.py
```

---

## ðŸ“‚ Project Structure
```
â”œâ”€â”€ Documentation
â”‚   â””â”€â”€ ToolComparison.docx
â”œâ”€â”€ __pycache__
â”‚   â”œâ”€â”€ auth.cpython-313.pyc
â”‚   â”œâ”€â”€ azurePdfScraping.cpython-312.pyc
â”‚   â”œâ”€â”€ azurePdfScraping.cpython-313.pyc
â”‚   â”œâ”€â”€ doclingScraping.cpython-312.pyc
â”‚   â”œâ”€â”€ doclingScraping.cpython-313.pyc
â”‚   â”œâ”€â”€ openSourcePdf.cpython-312.pyc
â”‚   â”œâ”€â”€ openSourcePdf.cpython-313.pyc
â”‚   â”œâ”€â”€ scrapingBee.cpython-312.pyc
â”‚   â”œâ”€â”€ scrapingBee.cpython-313.pyc
â”‚   â”œâ”€â”€ selenium.cpython-312.pyc
â”‚   â”œâ”€â”€ seleniumScraping.cpython-312.pyc
â”‚   â””â”€â”€ seleniumScraping.cpython-313.pyc
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ app.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ azurePdfScraping.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ doclingScraping.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ openSourcePdf.cpython-313.pyc
â”‚   â”‚   â”œâ”€â”€ scrapingBee.cpython-313.pyc
â”‚   â”‚   â””â”€â”€ seleniumScraping.cpython-313.pyc
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ azurePdfScraping.py
â”‚   â”œâ”€â”€ extracted_data.md
â”‚   â”œâ”€â”€ file.pdf
â”‚   â”œâ”€â”€ openSourcePdf.py
â”‚   â”œâ”€â”€ scrapingBee.py
â”‚   â””â”€â”€ seleniumScraping.py
â”œâ”€â”€ diagrams
â”‚   â”œâ”€â”€ ai_application_data_pipeline.png
â”‚   â”œâ”€â”€ architecture_diagram.ipynb
â”‚   â”œâ”€â”€ azure.png
â”‚   â”œâ”€â”€ docling.png
â”‚   â”œâ”€â”€ markitdown.png
â”‚   â”œâ”€â”€ pymupdf.png
â”‚   â”œâ”€â”€ scrapingbee.png
â”‚   â”œâ”€â”€ selenium.png
â”‚   â””â”€â”€ streamlit.png
â”œâ”€â”€ directory_structure.txt
â”œâ”€â”€ doclingScraping.py
â”œâ”€â”€ frontend
â”‚   â””â”€â”€ dashboard.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ venv
â”‚   â”œâ”€â”€ bin
â”‚   â”‚   â”œâ”€â”€ Activate.ps1
â”‚   â”‚   â”œâ”€â”€ __pycache__
â”‚   â”‚   â”œâ”€â”€ activate
â”‚   â”‚   â”œâ”€â”€ activate.csh
â”‚   â”‚   â”œâ”€â”€ activate.fish
â”‚   â”‚   â”œâ”€â”€ dotenv
â”‚   â”‚   â”œâ”€â”€ f2py
â”‚   â”‚   â”œâ”€â”€ fastapi
â”‚   â”‚   â”œâ”€â”€ jp.py
â”‚   â”‚   â”œâ”€â”€ jsonschema
â”‚   â”‚   â”œâ”€â”€ markdown-it
â”‚   â”‚   â”œâ”€â”€ normalizer
â”‚   â”‚   â”œâ”€â”€ numpy-config
â”‚   â”‚   â”œâ”€â”€ pip
â”‚   â”‚   â”œâ”€â”€ pip3
â”‚   â”‚   â”œâ”€â”€ pip3.13
â”‚   â”‚   â”œâ”€â”€ pygmentize
â”‚   â”‚   â”œâ”€â”€ pymupdf
â”‚   â”‚   â”œâ”€â”€ python -> python3.13
â”‚   â”‚   â”œâ”€â”€ python3 -> python3.13
â”‚   â”‚   â”œâ”€â”€ python3.13 -> /Library/Frameworks/Python.framework/Versions/3.13/bin/python3.13
â”‚   â”‚   â”œâ”€â”€ streamlit
â”‚   â”‚   â”œâ”€â”€ streamlit.cmd
â”‚   â”‚   â”œâ”€â”€ uvicorn
â”‚   â”‚   â””â”€â”€ wsdump
â”‚   â”œâ”€â”€ etc
â”‚   â”‚   â””â”€â”€ jupyter
â”‚   â”œâ”€â”€ include
â”‚   â”‚   â””â”€â”€ python3.13
â”‚   â”œâ”€â”€ lib
â”‚   â”‚   â””â”€â”€ python3.13
â”‚   â”œâ”€â”€ pyvenv.cfg
â”‚   â””â”€â”€ share
â”‚       â””â”€â”€ jupyter
â””â”€â”€ vercel.json

```

---

## ðŸ“¦ Final Deliverables
ðŸ“Œ **GitHub Repository** â€“ Code, documentation, and comparative analysis.
ðŸ“Œ **Comparative Report** â€“ Insights on trade-offs between open-source and enterprise tools.
ðŸ“Œ **5-minute Demo Video** â€“ Walkthrough of key findings and PoC.
ðŸ“Œ **AI Use Disclosure** â€“ Transparency report on AI tool usage.

---

## ðŸš€ Future Enhancements
- **Airflow Integration**: Automate scraping and extraction processes.
- **Chatbots driven by LLM**: Create a chatbot that responds to queries based on extracted documents.
- **Knowledge Base**: Develop a structured knowledge base from extracted document insights.

---

## ðŸ‘¥ Team Information
| Name            | Student ID    | Contribution |
|----------------|--------------|--------------|
| **Pranjal Mahajan** | 002375449  | 33.33% |
| **Srushti Patil**  | 002345025  | 33.33% |
| **Ram Putcha**  | 002304724  | 33.33% |

---

